"""
ç½‘ç«™å‘ç°ç³»ç»Ÿ - ä¸»è¦ä¸šåŠ¡é€»è¾‘æµç¨‹æ§åˆ¶
"""
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime
import time

from config_manager import ConfigManager
from unified_query_chain import create_unified_query_chain, QueryResult
from search_providers import create_search_provider, ConcurrentSearchManager
from content_processor import ContentCrawler, ContentScorer, ProcessedContent
from crawling_providers import EnhancedCrawlManager
from excel_exporter import ExcelExporter


class WebsiteDiscoveryEngine:
    """ç½‘ç«™å‘ç°å¼•æ“"""
    
    def __init__(self, config_path: str = "config.yaml"):
        self.config_manager = ConfigManager(config_path)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.llm = self.config_manager.get_llm()
        self.embeddings = self.config_manager.get_embeddings()
        self.search_provider = create_search_provider(self.config_manager.get_search_config())
        
        self.query_chain = create_unified_query_chain(self.llm, self.config_manager.config)
        
        # é…ç½®å‚æ•°
        self.runtime_config = self.config_manager.get_runtime_config()
        self.logic_config = self.config_manager.get_logic_config()
        self.scoring_weights = self.config_manager.get_scoring_weights()
        self.export_config = self.config_manager.get_export_config()
        
        # åˆå§‹åŒ–å¤„ç†ç»„ä»¶
        self.search_manager = ConcurrentSearchManager(
            self.search_provider,
            self.runtime_config.get('search_concurrency', 20)
        )
        
        # åˆå§‹åŒ–çˆ¬è™«ç®¡ç†å™¨
        try:
            self.crawl_manager = EnhancedCrawlManager(self.config_manager.config)
        except Exception as e:
            print(f"åˆå§‹åŒ–å¤–éƒ¨çˆ¬è™«å¤±è´¥ï¼Œä½¿ç”¨åŸç”Ÿçˆ¬è™«: {e}")
            self.crawl_manager = None
        
        self.crawler = ContentCrawler(
            concurrency=self.runtime_config.get('crawl_concurrency', 50),
            timeout=self.runtime_config.get('request_timeout_sec', 20),
            per_domain_rps=self.runtime_config.get('per_domain_rps', 1.0),
            crawl_manager=self.crawl_manager
        )
        
        self.scorer = ContentScorer(self.embeddings, self.scoring_weights)
        self.exporter = ExcelExporter(self.export_config)
    
    async def discover_websites(self, 
                              demand_text: str, 
                              seed_urls: Optional[List[str]] = None,
                              max_depth: int = None) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„ç½‘ç«™å‘ç°æµç¨‹"""
        
        start_time = time.time()
        seed_urls = seed_urls or []
        max_depth = max_depth or self.logic_config.get('max_depth', 2)
        
        print(f"ğŸš€ å¼€å§‹ç½‘ç«™å‘ç°ä»»åŠ¡")
        print(f"ğŸ“ ç”¨æˆ·éœ€æ±‚: {demand_text}")
        print(f"ğŸŒ ç§å­ç½‘ç«™æ•°é‡: {len(seed_urls)}")
        print(f"ğŸ“Š æœ€å¤§æ·±åº¦: {max_depth}")
        print()
        
        try:
            # ç¬¬1æ­¥ï¼šç»Ÿä¸€åˆ†æ - ç”Ÿæˆæœç´¢æŸ¥è¯¢
            print("ğŸ” ç¬¬1æ­¥ï¼šç”Ÿæˆæœç´¢æŸ¥è¯¢...")
            query_result = await self.query_chain.ainvoke({
                "demand_text": demand_text,
                "seed_urls": seed_urls
            })
            
            queries = query_result["queries"]
            coverage_tags = query_result["coverage_tags"]
            print(f"âœ… ç”Ÿæˆäº† {len(queries)} ä¸ªæœç´¢æŸ¥è¯¢")
            print(f"ğŸ“‹ è¦†ç›–æ ‡ç­¾: {', '.join(coverage_tags)}")
            print()
            
            if not queries:
                print("âŒ æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„æœç´¢æŸ¥è¯¢")
                return self._create_empty_result(demand_text, seed_urls, start_time)
            
            # ç¬¬2æ­¥ï¼šå¹¶å‘æœç´¢
            print("ğŸ” ç¬¬2æ­¥ï¼šæ‰§è¡Œå¹¶å‘æœç´¢...")
            query_strings = [q.query for q in queries]
            search_results = await self.search_manager.search_queries(query_strings, 10)
            print(f"âœ… æœç´¢åˆ° {len(search_results)} ä¸ªåˆå§‹ç»“æœ")
            print()
            
            if not search_results:
                print("âŒ æœç´¢æœªè¿”å›ä»»ä½•ç»“æœ")
                return self._create_empty_result(demand_text, seed_urls, start_time)
            
            # ç¬¬3æ­¥ï¼šæ‘˜è¦ç­›é€‰ï¼ˆåŸºäºç›¸ä¼¼åº¦å†³å®šæ˜¯å¦æŠ“å–è¯¦æƒ…ï¼‰
            print("ğŸ“‹ ç¬¬3æ­¥ï¼šæ‘˜è¦ç­›é€‰...")
            filtered_results = await self._filter_by_summary(
                search_results, demand_text, query_strings
            )
            print(f"âœ… ç­›é€‰åä¿ç•™ {len(filtered_results)} ä¸ªç»“æœè¿›è¡Œè¯¦æƒ…æŠ“å–")
            print()
            
            # ç¬¬4æ­¥ï¼šè¯¦æƒ…æŠ“å–
            print("ğŸ•·ï¸ ç¬¬4æ­¥ï¼šæŠ“å–è¯¦æƒ…å†…å®¹...")
            processed_contents = await self.crawler.crawl_urls(filtered_results)
            successful_contents = [c for c in processed_contents if c.content]
            print(f"âœ… æˆåŠŸæŠ“å– {len(successful_contents)} ä¸ªé¡µé¢å†…å®¹")
            print()
            
            if not successful_contents:
                print("âŒ å†…å®¹æŠ“å–å¤±è´¥ï¼Œæ— æœ‰æ•ˆå†…å®¹")
                return self._create_empty_result(demand_text, seed_urls, start_time)
            
            # ç¬¬5æ­¥ï¼šå†…å®¹è¯„åˆ†
            print("ğŸ“Š ç¬¬5æ­¥ï¼šå†…å®¹è¯„åˆ†...")
            keywords = self._extract_keywords_from_queries(queries)
            scored_contents = await self.scorer.score_contents(
                successful_contents, demand_text, keywords
            )
            print(f"âœ… å®Œæˆ {len(scored_contents)} ä¸ªå†…å®¹çš„è¯„åˆ†")
            print()
            
            # ç¬¬6æ­¥ï¼šå†³ç­–æ ‡è®°
            print("âœ… ç¬¬6æ­¥ï¼šåº”ç”¨å†³ç­–é˜ˆå€¼...")
            final_contents = self._apply_decision_threshold(scored_contents)
            accepted_count = len([c for c in final_contents if c.decision == 'accepted'])
            print(f"âœ… æ¥å— {accepted_count} ä¸ªé«˜è´¨é‡ç»“æœ")
            print()
            
            # ç¬¬7æ­¥ï¼šä¸‹é’»å¤„ç†ï¼ˆå¦‚æœéœ€è¦ä¸”æ·±åº¦å…è®¸ï¼‰
            if max_depth > 1:
                print("ğŸ”— ç¬¬7æ­¥ï¼šä¸‹é’»ç›¸å…³é“¾æ¥...")
                final_contents = await self._process_drill_down(
                    final_contents, demand_text, keywords, max_depth
                )
                print(f"âœ… ä¸‹é’»åæ€»è®¡ {len(final_contents)} ä¸ªç»“æœ")
                print()
            
            # ç¬¬8æ­¥ï¼šå¯¼å‡ºExcel
            print("ğŸ“Š ç¬¬8æ­¥ï¼šå¯¼å‡ºExcelæŠ¥å‘Š...")
            execution_time = time.time() - start_time
            
            task_metadata = {
                'execution_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'config_path': self.config_manager.config_path,
                'total_time_seconds': round(execution_time, 2),
                'search_provider': self.config_manager.get_search_config()['provider'],
                'llm_provider': self.config_manager.config['providers']['llm']['provider']
            }
            
            excel_path = self.exporter.export_results(
                final_contents, queries, demand_text, coverage_tags, task_metadata
            )
            
            # ç”Ÿæˆç®€åŒ–CSVæŠ¥å‘Š
            csv_path = self.exporter.create_simple_report(final_contents)
            
            print(f"âœ… ä»»åŠ¡å®Œæˆï¼è€—æ—¶ {execution_time:.2f} ç§’")
            print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {excel_path}")
            print(f"ğŸ“„ ç®€åŒ–æŠ¥å‘Š: {csv_path}")
            
            return {
                'success': True,
                'demand_text': demand_text,
                'seed_urls': seed_urls,
                'queries_generated': len(queries),
                'search_results': len(search_results),
                'successful_crawls': len(successful_contents),
                'accepted_results': accepted_count,
                'total_results': len(final_contents),
                'execution_time': execution_time,
                'excel_path': excel_path,
                'csv_path': csv_path,
                'coverage_tags': coverage_tags,
                'contents': final_contents,
                'queries': queries,
                'task_metadata': task_metadata
            }
            
        except Exception as e:
            print(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                'success': False,
                'error': str(e),
                'demand_text': demand_text,
                'seed_urls': seed_urls,
                'execution_time': time.time() - start_time
            }
    
    async def _filter_by_summary(self, search_results, demand_text: str, queries: List[str]):
        """åŸºäºæ‘˜è¦è¿›è¡Œç­›é€‰"""
        threshold = self.logic_config.get('detail_threshold', 0.55)
        
        # ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦ç­›é€‰ï¼ˆåŸºäºå…³é”®è¯å‡ºç°ï¼‰
        keywords = []
        for query in queries:
            # æå–æŸ¥è¯¢ä¸­çš„å…³é”®è¯ï¼ˆå»é™¤æ“ä½œç¬¦ï¼‰
            clean_query = query.lower()
            for op in ['site:', 'intitle:', 'inurl:', 'filetype:', 'and', 'or', '"', '(', ')', '-']:
                clean_query = clean_query.replace(op, ' ')
            keywords.extend(clean_query.split())
        
        keywords = list(set([kw.strip() for kw in keywords if len(kw.strip()) > 2]))
        
        filtered_results = []
        for result in search_results:
            # è®¡ç®—æ‘˜è¦ä¸­å…³é”®è¯å‡ºç°æ¯”ä¾‹
            snippet_lower = (result.title + " " + result.snippet).lower()
            matched_keywords = sum(1 for kw in keywords if kw in snippet_lower)
            keyword_ratio = matched_keywords / len(keywords) if keywords else 0
            
            if keyword_ratio >= threshold or matched_keywords >= 3:  # è‡³å°‘åŒ¹é…3ä¸ªå…³é”®è¯
                filtered_results.append(result)
        
        return filtered_results
    
    def _extract_keywords_from_queries(self, queries: List[QueryResult]) -> List[str]:
        """ä»æŸ¥è¯¢ä¸­æå–å…³é”®è¯"""
        keywords = []
        for query in queries:
            clean_query = query.query.lower()
            # ç§»é™¤æœç´¢æ“ä½œç¬¦
            for op in ['site:', 'intitle:', 'inurl:', 'filetype:', 'and', 'or', '"', '(', ')', '-']:
                clean_query = clean_query.replace(op, ' ')
            
            words = [word.strip() for word in clean_query.split() if len(word.strip()) > 2]
            keywords.extend(words)
        
        return list(set(keywords))
    
    def _apply_decision_threshold(self, contents: List[ProcessedContent]) -> List[ProcessedContent]:
        """åº”ç”¨å†³ç­–é˜ˆå€¼"""
        threshold = self.logic_config.get('score_threshold', 0.75)
        
        for content in contents:
            if content.final_score >= threshold:
                content.decision = 'accepted'
                content.explanation = f'é«˜åˆ†å†…å®¹ (è¯„åˆ†: {content.final_score:.4f})'
            elif content.final_score >= 0.5:
                content.decision = 'candidate'
                content.explanation = f'å€™é€‰å†…å®¹ (è¯„åˆ†: {content.final_score:.4f})'
            else:
                content.decision = 'rejected'
                content.explanation = f'ä½åˆ†å†…å®¹ (è¯„åˆ†: {content.final_score:.4f})'
        
        return contents
    
    async def _process_drill_down(self, contents: List[ProcessedContent], 
                                demand_text: str, keywords: List[str], 
                                max_depth: int) -> List[ProcessedContent]:
        """å¤„ç†ä¸‹é’»é“¾æ¥"""
        if max_depth <= 1:
            return contents
        
        # é€‰æ‹©é«˜åˆ†å†…å®¹è¿›è¡Œä¸‹é’»
        high_score_contents = [c for c in contents if c.final_score > 0.6]
        
        drill_down_urls = []
        max_links = self.logic_config.get('max_links_per_page', 30)
        
        for content in high_score_contents[:5]:  # é™åˆ¶ä¸‹é’»æ•°é‡
            relevant_links = []
            for link in content.extracted_links[:max_links]:
                # ç®€å•çš„ç›¸å…³æ€§åˆ¤æ–­
                link_lower = link.lower()
                if any(kw in link_lower for kw in keywords[:10]):  # ä½¿ç”¨å‰10ä¸ªå…³é”®è¯
                    relevant_links.append(link)
            
            drill_down_urls.extend(relevant_links[:5])  # æ¯é¡µæœ€å¤š5ä¸ªé“¾æ¥
        
        if not drill_down_urls:
            return contents
        
        print(f"ğŸ”— ä¸‹é’»å¤„ç† {len(drill_down_urls)} ä¸ªç›¸å…³é“¾æ¥...")
        
        # å°†ä¸‹é’»URLè½¬æ¢ä¸ºæœç´¢ç»“æœæ ¼å¼
        from search_providers import SearchResult
        drill_search_results = []
        for i, url in enumerate(drill_down_urls):
            drill_search_results.append(SearchResult(
                title="",
                url=url,
                snippet="",
                source_query="drill_down",
                rank=i + 1
            ))
        
        # æŠ“å–ä¸‹é’»å†…å®¹
        drill_contents = await self.crawler.crawl_urls(drill_search_results)
        successful_drill = [c for c in drill_contents if c.content]
        
        if successful_drill:
            # è®¾ç½®æ·±åº¦æ ‡è®°
            for content in successful_drill:
                content.depth = 1
                content.parent_url = "drill_down"
            
            # è¯„åˆ†
            scored_drill = await self.scorer.score_contents(
                successful_drill, demand_text, keywords
            )
            
            # åº”ç”¨å†³ç­–
            final_drill = self._apply_decision_threshold(scored_drill)
            
            # åˆå¹¶ç»“æœ
            contents.extend(final_drill)
        
        return contents
    
    def _create_empty_result(self, demand_text: str, seed_urls: List[str], start_time: float) -> Dict[str, Any]:
        """åˆ›å»ºç©ºç»“æœ"""
        return {
            'success': False,
            'demand_text': demand_text,
            'seed_urls': seed_urls,
            'queries_generated': 0,
            'search_results': 0,
            'successful_crawls': 0,
            'accepted_results': 0,
            'total_results': 0,
            'execution_time': time.time() - start_time,
            'error': 'No results found'
        }